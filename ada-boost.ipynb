{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "from DecisionTree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('heart.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial rows in in dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the Dataset\n",
    "print(\"Dimensions of the Dataset (Rows, Columns):\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical summary of numeric features:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical summary of categorical features:\")\n",
    "df.describe(include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any leading, and trailing whitespaces in columns\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nulls\n",
    "null = df.isna().sum()\n",
    "total = len(df)\n",
    "percentage = (null / total) * 100\n",
    "print(percentage[percentage != 0])\n",
    "print(f\"The percentage of nulls in each column:\\n{percentage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RestingBP'].replace(0, np.nan, inplace=True)\n",
    "df['Cholesterol'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nulls\n",
    "null = df.isna().sum()\n",
    "total = len(df)\n",
    "percentage = (null / total) * 100\n",
    "print(percentage[percentage != 0])\n",
    "print(f\"The percentage of nulls in each column:\\n{percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Use KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[['RestingBP', 'Cholesterol']] = imputer.fit_transform(df[['RestingBP', 'Cholesterol']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to the visualization of numeric & categorical features with the target feature\n",
    "def num_features(df, num_var, tar_var):\n",
    "    x0 = df[df[tar_var]==0][num_var]\n",
    "    x1 = df[df[tar_var]==1][num_var]\n",
    "\n",
    "    trace1 = go.Histogram(x = x0,\n",
    "                               name = \"No\",\n",
    "                               opacity = 0.75,\n",
    "                               marker=dict(color=\"#0A4D68\",\n",
    "                                           line = dict(color = '#DBE6EC',\n",
    "                                                       width = 1)))\n",
    "\n",
    "    trace2 = go.Histogram(x = x1,\n",
    "                               name = \"Yes\",\n",
    "                               opacity = 0.75,\n",
    "                               marker=dict(color=\"#A6D0DD\",\n",
    "                                           line = dict(color = '#DBE6EC',\n",
    "                                                       width = 1)))\n",
    "\n",
    "    data = [trace1, trace2]\n",
    "\n",
    "    layout = go.Layout(title={'text': num_var,\n",
    "                         'y':0.9,\n",
    "                         'x':0.5,\n",
    "                         'xanchor':'center',\n",
    "                         'yanchor':'top'},\n",
    "                         barmode='overlay',\n",
    "                         yaxis=dict(title='Count'),\n",
    "                         template = 'plotly_dark')\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HeartDisease'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cholesterol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.describe().columns:\n",
    "  num_features(df, i, \"HeartDisease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating Numeric and Categorical Columns\n",
    "numeric_col = [col for col in df.columns if df[col].dtype != 'object']\n",
    "categorical_col = [col for col in df.columns if df[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Categorical features\n",
    "for feature in categorical_col:\n",
    "    num_features(df, feature, \"HeartDisease\")\n",
    "    print(\"\\n {}\".format(df[feature].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Outliers by boxplots\n",
    "for col in numeric_col:\n",
    "   plt.figure(figsize=(8, 6))\n",
    "   sns.boxplot(data= df[col])\n",
    "   plt.title(f'{col} Box plot')\n",
    "   #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df[numeric_col].corr()\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for column in df.select_dtypes(include=['object']):\n",
    "    df[column] = le.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(['HeartDisease'],axis=1)\n",
    "Y=df[\"HeartDisease\"] # Target\n",
    "\n",
    "print(f\"the shape of the inputs x is: {X.shape}\")\n",
    "print(f\"the shape of the targets y is: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.where(Y == 0, -1, 1)  \n",
    "\n",
    "# Split dataset: 60% train, 20% validation, 20% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying with multible scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scalers\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decsision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over different scalers\n",
    "for name, scaler in scalers.items():\n",
    "    print(f\"\\n=== Using {name} ===\")\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train AdaBoost model\n",
    "    model = DecisionTreeClassifier(max_depth=100)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Final prediction on test set\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[-1, 1], yticklabels=[-1, 1])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix ({name})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class Bagging:\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = []\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeClassifier()  # Base model\n",
    "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.models])\n",
    "        majority_votes = np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return majority_votes\n",
    "\n",
    "\n",
    "# Iterate over different scalers\n",
    "for name, scaler in scalers.items():\n",
    "    print(f\"\\n=== Using {name} ===\")\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train Bagging model\n",
    "    model = Bagging(n_estimators=10)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "\n",
    "    # Final prediction on test set\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix ({name})\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        n_samples, n_features = X.shape\n",
    "        w = np.ones(n_samples) / n_samples  # Initialize sample weights\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            stump = DecisionTreeClassifier(max_depth=1)\n",
    "            stump.fit(X, Y, sample_weight=w)\n",
    "            y_pred = stump.predict(X)\n",
    "\n",
    "            err = np.sum(w * (y_pred != Y)) / np.sum(w)  # Compute error\n",
    "            alpha = 0.5 * np.log((1 - err) / (err + 1e-10))  # Model weight\n",
    "            \n",
    "            w *= np.exp(-alpha * Y * y_pred)  # Update sample weights\n",
    "            w /= np.sum(w)  # Normalize\n",
    "            \n",
    "            self.models.append(stump)\n",
    "            self.alphas.append(alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        weak_preds = np.array([alpha * model.predict(X) for model, alpha in zip(self.models, self.alphas)])\n",
    "        return np.sign(np.sum(weak_preds, axis=0))  # Aggregate predictions\n",
    "\n",
    "# Iterate over different scalers\n",
    "for name, scaler in scalers.items():\n",
    "    print(f\"\\n=== Using {name} ===\")\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train AdaBoost model\n",
    "    model = AdaBoost(n_estimators=50)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Final prediction on test set\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[-1, 1], yticklabels=[-1, 1])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix ({name})\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define different scalers\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler()\n",
    "}\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"n_neighbors\": [3, 5, 7, 11],  # Different values of k\n",
    "    \"metric\": [\"euclidean\", \"manhattan\"]  # Distance metrics\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_f1 = 0\n",
    "best_scaler_name = None\n",
    "\n",
    "# Iterate over different scalers\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    # Scale the data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Hyperparameter tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring=\"f1_weighted\", n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    best_knn = grid_search.best_estimator_\n",
    "    y_val_pred = best_knn.predict(X_val_scaled)\n",
    "    f1 = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"Scaler: {scaler_name} | Best k: {grid_search.best_params_['n_neighbors']} | Best Metric: {grid_search.best_params_['metric']} | F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Store the best model\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = best_knn\n",
    "        best_scaler_name = scaler_name\n",
    "        best_scaler = scaler  # Save the best scaler\n",
    "\n",
    "# Apply best scaler to test data\n",
    "X_test_scaled = best_scaler.transform(X_test)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred, average=\"weighted\")  # Change to 'macro' or 'binary' as needed\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nBest Model Performance on Test Set:\")\n",
    "print(f\"Best Scaler: {best_scaler_name}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", xticklabels=np.unique(Y), yticklabels=np.unique(Y))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define different scalers\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler()\n",
    "}\n",
    "\n",
    "# Define hyperparameter grid for logistic regression\n",
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "best_model = None\n",
    "best_f1 = 0\n",
    "best_scaler_name = None\n",
    "\n",
    "# Iterate over different scalers\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    # Scale the data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Hyperparameter tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), param_grid, cv=5, scoring=\"f1_weighted\", n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    best_log_reg = grid_search.best_estimator_\n",
    "    y_val_pred = best_log_reg.predict(X_val_scaled)\n",
    "    f1 = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"Scaler: {scaler_name} | Best C: {grid_search.best_params_['C']} | F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Store the best model\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = best_log_reg\n",
    "        best_scaler_name = scaler_name\n",
    "        best_scaler = scaler  # Save the best scaler\n",
    "\n",
    "# Apply best scaler to test data\n",
    "X_test_scaled = best_scaler.transform(X_test)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred, average=\"weighted\")  # Change to 'macro' or 'binary' as needed\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nBest Model Performance on Test Set:\")\n",
    "print(f\"Best Scaler: {best_scaler_name}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", xticklabels=np.unique(Y), yticklabels=np.unique(Y))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42)\n",
    "\n",
    "# Define different scalers\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler()\n",
    "}\n",
    "\n",
    "# Define hyperparameter options\n",
    "hidden_layer_sizes = [16, 32]\n",
    "learning_rates = [0.001, 0.01]\n",
    "epochs_list = [50, 100]\n",
    "\n",
    "best_model = None\n",
    "best_f1 = 0\n",
    "best_scaler_name = None\n",
    "best_hparams = {}\n",
    "\n",
    "# Iterate over different scalers\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Iterate over hyperparameter combinations\n",
    "    for hidden_size in hidden_layer_sizes:\n",
    "        for lr in learning_rates:\n",
    "            for epochs in epochs_list:\n",
    "                # Build neural network model\n",
    "                model = keras.Sequential([\n",
    "                    keras.layers.Dense(hidden_size, activation=\"relu\", input_shape=(X_train_scaled.shape[1],)),\n",
    "                    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "                ])\n",
    "\n",
    "                # Compile the model\n",
    "                model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                              loss=\"binary_crossentropy\",\n",
    "                              metrics=[\"accuracy\"])\n",
    "\n",
    "                # Train the model\n",
    "                history = model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val),\n",
    "                                    epochs=epochs, batch_size=32, verbose=0)\n",
    "\n",
    "                # Predict on validation set\n",
    "                y_val_pred_probs = model.predict(X_val_scaled)\n",
    "                y_val_pred = (y_val_pred_probs > 0.5).astype(int)\n",
    "\n",
    "                # Compute F1-score\n",
    "                f1 = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "\n",
    "                print(f\"Scaler: {scaler_name} | Hidden Layer: {hidden_size} | LR: {lr} | Epochs: {epochs} | F1: {f1:.4f}\")\n",
    "\n",
    "                # Store the best model\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_model = model\n",
    "                    best_scaler_name = scaler_name\n",
    "                    best_scaler = scaler\n",
    "                    best_hparams = {\"hidden_layer\": hidden_size, \"learning_rate\": lr, \"epochs\": epochs}\n",
    "\n",
    "# Apply best scaler to test data\n",
    "X_test_scaled = best_scaler.transform(X_test)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_probs = best_model.predict(X_test_scaled)\n",
    "y_test_pred = (y_test_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest Model Performance on Test Set:\")\n",
    "print(f\"Best Scaler: {best_scaler_name}\")\n",
    "print(f\"Best Hyperparameters: {best_hparams}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", xticklabels=np.unique(Y), yticklabels=np.unique(Y))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1582403,
     "sourceId": 2603715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
